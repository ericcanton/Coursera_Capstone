{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and Clustering of Toronto Neighborhoods\n",
    "This notebook will do some segmentation of Toronto neighborhoods. Eric Canton wrote this as part of completing the IBM Data Science Capstone. \n",
    "1. Initially, we will get geographic data from <a href=\"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\">this Wikipedia table</a> using Wikipedia's REST API. These data are loaded into a DataFrame.\n",
    "2. We will then use the Foursquare API to gather some info about local coffee shops (excluding big names like Starbucks, Dunkin, and Caribou) and gyms by postal code. \n",
    "3. These data we will combine to cluster neighborhoods by similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# JSON and REST API\n",
    "import requests # Python library for HTTP/API requests.\n",
    "import json\n",
    "from pandas.io.json import json_normalize # Transform JSON into DataFrames\n",
    "\n",
    "# Clustering and map generation\n",
    "import folium\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Get the table JSON from Wikipedia and parse it into a DataFrame\n",
    "Wikipedia has a <a href=\"https://www.mediawiki.org/wiki/API:Main_page\">really helpful description of their API</a> which is pretty easy to understand.  \n",
    "\n",
    "One does not need to authenticate to use their API for information requests. The main reference I used was the <a href=\"https://www.mediawiki.org/wiki/API:Parsing_wikitext\">API: Parsing wikitext page</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the url and request parameters\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Specifying prop=wikitext gives the simpler editing format used when editing Wikipedia source. \n",
    "PARAMS = {\n",
    "    \"action\" : \"parse\",\n",
    "    \"page\"   : \"List_of_postal_codes_of_Canada:_M\",\n",
    "    \"prop\"   : \"wikitext\",\n",
    "    \"format\" : \"json\"\n",
    "}\n",
    "\n",
    "M = requests.get(url=URL, params=PARAMS).json()\n",
    "wiki_text = M['parse']['wikitext']['*'] # The wikitext source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and parse the table.\n",
    "By either looking at the JSON, or navigating to <a href=\"https://en.wikipedia.org/w/index.php?title=List_of_postal_codes_of_Canada:_M&action=edit&section=1\">[edit source]</a> on the Wikipedia page, we can see that the table is delineated by <code>{| .... |}.</code> Our strategy for finding the table, then splitting it into rows and further parsing each row, is the following.\n",
    "1. Use the <code>wiki_text.find</code> method to locate the table (as the text between {| and |}) and trim the wikitext down into a new string called <code>table</code>.\n",
    "2. Looking more closely at <code>table</code>, we see that the entries in the table are split up by <code>\\n|-\\n|</code>. We can use the <code>split</code> method to cut <code>table</code> into its <code>rows</code>. \n",
    "3. Each row has a very reliable format (see the output to the cell below) making it easy to further drop any rows missing a borough, then <code>split</code> the rows into their fields.  \n",
    "4. Once we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Postcode !! Borough !! Neighbourhood\n",
      " M1A || Not assigned || Not assigned\n"
     ]
    }
   ],
   "source": [
    "table_start = wiki_text.find(\"{|\")+2 # skip the leading {|\n",
    "table_end = wiki_text.find(\"|}\") # string slicing will STOP at this index-1, so this is perfect.\n",
    "table = wiki_text[table_start:table_end] # slice the string to get just those characters defining the table. \n",
    "rows = table.split(\"\\n|-\\n|\") # split the table into its rows, discarding the newlines and |- to give just the intersting stuff.\n",
    "\n",
    "# Get the column names, which we'll use to form our DataFrame later on.\n",
    "cols = rows[0] \n",
    "cols = cols[cols.find(\"!\"):] # There's actually one line before the columns, class=\"wikitable sortable\", which we discard.\n",
    "\n",
    "# Now just keep the actual records. \n",
    "rows = rows[1:] \n",
    "\n",
    "print(cols)\n",
    "print(rows[0]) # take a look at the format of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "Per the instructions for this assignment, we want to throw away every entry that has \"Not assigned\" in borough, which is the first column after the postal code.\n",
    "\n",
    "Since the <code>find</code> method returns the index where it first discovers a substring, or -1 if the substring isn't found, we just keep those rows for which <code>find</code> returns -1. No row contains a neighborhood name but not a borough name, so we avoid those rows containing <code>Not assigned || Not assigned</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [r for r in rows if r.find(\"Not assigned || Not assigned\") == -1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the column names and process the rows into a new array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols[2:].split(\" !! \") # Start slicing at index 2 since cols starts like \"! Postal...\"\n",
    "\n",
    "records = []\n",
    "for r in rows:\n",
    "    rec = r.split(' || ')\n",
    "    \n",
    "    # Some of the records have [[...]] around place names, because Wikipedia has a page on that place and they want to give a link. \n",
    "    # This makes Wikipedia great, but is not what we want. Get rid of these as we go.\n",
    "    for i in range(3):\n",
    "        rec[i] = rec[i][2:-2] if (rec[i].find(\"[[\") > -1) else rec[i]\n",
    "    records.append(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DataFrame\n",
    "To close out this section, we're going to make the DataFrame from our parsed rows. My preferred method for this is to first create a dictionary, which is then easy to cast to a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {c : [] for c in cols} # Set up a dict with keys we got from the table columns.\n",
    "\n",
    "for rec in records:\n",
    "    cols['Postcode'].append(rec[0])\n",
    "    cols['Borough'].append(rec[1])\n",
    "    if rec[2] == \"Not assigned\": # For the assignment, we should re-use borough name for neighborhood if \"Not assigned\"\n",
    "        cols['Neighbourhood'].append(rec[1]) \n",
    "    else:\n",
    "        cols['Neighbourhood'].append(rec[2])\n",
    "        \n",
    "M = pd.DataFrame.from_dict(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M3A</td>\n",
       "      <td>North York</td>\n",
       "      <td>Parkwoods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M4A</td>\n",
       "      <td>North York</td>\n",
       "      <td>Victoria Village</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M5A</td>\n",
       "      <td>Downtown Toronto</td>\n",
       "      <td>Regent Park|Harbourfront</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M6A</td>\n",
       "      <td>North York</td>\n",
       "      <td>Lawrence Heights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M6A</td>\n",
       "      <td>North York</td>\n",
       "      <td>Lawrence Manor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Postcode           Borough             Neighbourhood\n",
       "0      M3A        North York                 Parkwoods\n",
       "1      M4A        North York          Victoria Village\n",
       "2      M5A  Downtown Toronto  Regent Park|Harbourfront\n",
       "3      M6A        North York          Lawrence Heights\n",
       "4      M6A        North York            Lawrence Manor"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols[\"Postcode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols[\"Borough\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols[\"Neighbourhood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
